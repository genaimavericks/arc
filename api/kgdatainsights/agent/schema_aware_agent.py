import os
import json
import time
import hashlib
import threading
import traceback
import re
from pathlib import Path
from uuid import uuid4
from typing import Dict, Any

from dotenv import load_dotenv
from langchain.chains import GraphCypherQAChain
from langchain.graphs import Neo4jGraph
from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain_neo4j import (
    Neo4jChatMessageHistory
)

from .templates.foamfactory.cypher_prompt_template import CYPHER_GENERATION_PROMPT
from .templates.foamfactory.qa_prompt_template import QA_PROMPT
from .cache import Cache
from .cache import cacheable

from ...kginsights.database_api import get_database_config, parse_connection_params
from ...models import Schema
from ...db_config import SessionLocal  
from .csv_to_cypher_generator import CsvToCypherGenerator
from ...utils.llm_provider import LLMProvider, LLMConstants

# Load environment variables
load_dotenv()

# Output directories
OUTPUT_DIR = Path("runtime-data/output/kgdatainsights")
SCHEMA_DIR = OUTPUT_DIR / "schema"
PROMPT_DIR = OUTPUT_DIR / "prompts"
QUERY_DIR = OUTPUT_DIR / "queries"  # This matches QUERIES_DIR in data_insights_api.py

# Create directories if they don't exist
SCHEMA_DIR.mkdir(parents=True, exist_ok=True)
PROMPT_DIR.mkdir(parents=True, exist_ok=True)

class SchemaAwareGraphAssistant:
    """
    Enhanced Graph Assistant that automatically manages schemas and prompts.
    Extends the functionality of Neo4jGraphChatAssistant by adding schema and prompt management.
    """
    def __init__(self, db_id: str, schema_id: str, schema: str, session_id: str = None):
        """
        Initialize the Schema-Aware Graph Assistant.
        
        Args:
            db_id: The ID of the Neo4j database to query
            schema_id: The ID of the schema being used
            schema: The schema content as a string or dict
            session_id: Optional session ID for chat history, generated if not provided
        """

        # Initialize LLM first
        #self.llm = LLMProvider.get_default_llm(temperature=0.0)
        self.llm = LLMProvider.get_llm(provider_name=LLMConstants.Providers.OPENAI, model_name=LLMConstants.OpenAIModels.DEFAULT, temperature=0.0)

        self.db_id = db_id
        self.schema_id = schema_id
        self.schema = json.loads(schema) if isinstance(schema, str) else schema
        self.session_id = session_id or f"session_{uuid4()}"
        self.formatted_schema = None
        # read csv path from Schema table
        self.csv_file_path = self._get_csv_path()
        self._format_schema()
        
        # Ensure we have the prompt files
        self._ensure_prompt()
        
        # Get Neo4j connection parameters from the database API
        self.connection_params = self._get_connection_params()
        print(f"Connection params: {self.connection_params}")
        
        # Initialize Neo4j Graph connection
        self.graph = Neo4jGraph(
            url=self.connection_params.get("uri"),
            username=self.connection_params.get("username"),
            password=self.connection_params.get("password"),
            database=self.connection_params.get("database", "neo4j"),
            enhanced_schema=True,
            refresh_schema=False  # Disable schema refresh to avoid APOC dependency
        )
        
        # Initialize Neo4j-backed chat history
        self.history = Neo4jChatMessageHistory(
            session_id=self.session_id,
            url=self.connection_params.get("uri"),
            username=self.connection_params.get("username"),
            password=self.connection_params.get("password"),
            database=self.connection_params.get("database", "neo4j")
        )
        
        # Load the custom prompts for this source
        self._load_prompts()
        
        # Initialize QA chain with Neo4j optimizations using modular LangChain pattern
        try:
            print(f"DEBUG: Initializing GraphCypherQAChain using langchain_neo4j pattern")
            
            # Create a structured chain configuration dictionary
            chain_config = {
                "llm": self.llm,
                "graph": self.graph,
                "verbose": True,
                "return_intermediate_steps": True,
                "allow_dangerous_requests": True
            }
            
            # Only add prompts if they are properly loaded
            if hasattr(self, 'cypher_prompt') and self.cypher_prompt:
                print('DEBUG: cypher prompt found')
                chain_config["cypher_prompt"] = self.cypher_prompt
            
            # QA prompt requires special handling because it relies on intermediate variables
            # that are generated by the chain itself (query and response)
            if hasattr(self, 'qa_prompt') and self.qa_prompt:
                print('DEBUG: qa prompt found - wrapping to handle intermediate variables')
                
                # We need to modify how the QA prompt is passed to work with GraphCypherQAChain
                # The chain internally generates 'query' and 'response' variables
                # Get the template string from our ChatPromptTemplate
                if hasattr(self.qa_prompt, 'template') and hasattr(self.qa_prompt.template, 'template'):
                    qa_template_str = self.qa_prompt.template.template
                    print(f"DEBUG: Using template string: {qa_template_str[:50]}...")
                    
                    # Create a proper PromptTemplate that GraphCypherQAChain can work with
                    # Based on the error, make sure we include 'query' as an input variable
                    # The langchain_neo4j GraphCypherQAChain specifically expects 'query' not 'question'
                    qa_prompt_template = PromptTemplate(
                        template=qa_template_str,
                        # Both 'query' and 'question' can refer to the same user input
                        # but the chain uses 'query' internally for the user question and for the Cypher query
                        input_variables=["query", "context"]
                    )
                    chain_config["qa_prompt"] = qa_prompt_template
                else:
                    print(f"DEBUG: Using original QA prompt - may cause errors")
                    chain_config["qa_prompt"] = self.qa_prompt
            
            # Initialize with the validated configuration
            self.chain = GraphCypherQAChain.from_llm(**chain_config)
            print(f"DEBUG: Successfully initialized GraphCypherQAChain")
            
        except Exception as e:
            print(f"ERROR: GraphCypherQAChain initialization failed: {str(e)}")
            print(f"DEBUG: Error details: {traceback.format_exc()}")        
        
        # Initialize cache
        self.cache = Cache()

    def _get_csv_path(self):
        """
        Read csv path from Schema table
        """
        db = SessionLocal()
        schema = db.query(Schema).filter(Schema.id == self.schema_id).first()
        return schema.csv_file_path

    def _get_connection_params(self) -> dict:
        """
        Get connection parameters for the specified database ID from the database API.
        
        Returns:
            dict: Connection parameters including uri, username, password, and database
        """
        try:
            # Get the full database configuration
            config = get_database_config()
            
            # Get the specific graph configuration
            graph_config = config.get(self.db_id, {})
            
            if not graph_config:
                print(f"WARNING: No configuration found for database '{self.db_id}' in neo4j.databases.yaml")
                # Try to use default configuration if available
                graph_config = config.get("default", {})
                if graph_config:
                    print(f"INFO: Using 'default' Neo4j configuration as fallback for '{self.db_id}'")
                else:
                    print(f"ERROR: No fallback configuration found for '{self.db_id}'")
            
            # Parse the connection parameters
            params = parse_connection_params(graph_config)
            
            # Validate and log the parameters
            if not params:
                print(f"ERROR: Failed to parse connection parameters for '{self.db_id}'")
                return {
                    "uri": None,
                    "username": None,
                    "password": None,
                    "database": None
                }
            
            # Log the connection parameters (hide password)
            conn_debug = {
                "uri": params.get("uri"),
                "username": params.get("username"),
                "database": params.get("database"),
                "password": "*****" if params.get("password") else None
            }
            print(f"DEBUG: Neo4j connection parameters for '{self.db_id}': {conn_debug}")
            
            # Validate the essential parameters
            if not params.get("uri"):
                print(f"ERROR: Missing Neo4j URI for database '{self.db_id}'")
            
            return params
            
        except Exception as e:
            print(f"ERROR: Failed to get connection params for {self.db_id}: {str(e)}")
            print(f"DEBUG: Stack trace: {traceback.format_exc()}")
            return {
                "uri": None,
                "username": None,
                "password": None,
                "database": None
            }
        
    def _ensure_prompt(self) -> None:
        """Check if prompts exist, validate generation_id, create and save if needed."""
        prompt_file = PROMPT_DIR / f"prompt_{self.db_id}_{self.schema_id}.json"
        query_file = QUERY_DIR / f"{self.schema_id}_queries.json" 

        regenerate_prompts = False
        current_generation_id = None
        existing_prompts = None

        # 1. Fetch current generation_id from DB
        db = None # Initialize db to None
        try:
            db = SessionLocal()
            schema_record = db.query(Schema).filter(Schema.id == self.schema_id).first()
            if schema_record:
                current_generation_id = schema_record.generation_id
                if not current_generation_id:
                    print(f"Warning: Schema record found for {self.schema_id}, but generation_id is missing. Will regenerate prompts.")
                    regenerate_prompts = True
                else:
                    print(f"DEBUG: Schema record found for {self.schema_id} with generation_id {current_generation_id}")    
            else:
                print(f"Warning: Schema record not found for schema_id {self.schema_id}. Cannot verify prompt generation ID. Will regenerate prompts.")
                regenerate_prompts = True # If schema record is missing, prompts might be invalid.
        except Exception as e:
            print(f"Error fetching schema generation_id: {e}. Proceeding, may regenerate prompts.")
            # If DB access fails, we should probably regenerate prompts
            regenerate_prompts = True
        finally:
            if db:
                db.close()

        # 2. Check prompt file and generation_id if regeneration not already decided
        if not regenerate_prompts and prompt_file.exists():
            try:
                with open(prompt_file, "r") as f:
                    existing_prompts = json.load(f)
                
                file_generation_id = existing_prompts.get("generation_id")

                if not current_generation_id: 
                     # This case should be handled above by setting regenerate_prompts=True
                     print(f"Error: Logic flaw - current_generation_id missing but regeneration wasn't triggered.")
                     regenerate_prompts = True 
                elif file_generation_id == current_generation_id:
                    print(f"Prompt file {prompt_file} found and generation_id matches. Loading prompts.")
                    # Load prompts later, set flag to false
                    regenerate_prompts = False 
                else:
                    print(f"Prompt file {prompt_file} found, but generation_id mismatch (File: {file_generation_id}, DB: {current_generation_id}). Regenerating prompts.")
                    regenerate_prompts = True
            except (json.JSONDecodeError, KeyError, Exception) as e:
                print(f"Error reading or parsing prompt file {prompt_file}: {e}. Regenerating prompts.")
                regenerate_prompts = True
        elif not prompt_file.exists():
             print(f"Prompt file for db '{self.db_id}' and schema '{self.schema_id}' not found. Creating prompts...")
             regenerate_prompts = True

        # 3. Regenerate and save if needed
        if regenerate_prompts:
            print("Generating new prompts...")
            # Generate prompts based on schema
            prompts = self._generate_prompts() # Assumes this returns a dict
            
            # Add the current generation_id (if available and valid)
            if current_generation_id:
                prompts["generation_id"] = current_generation_id
            else:
                 print(f"Warning: Could not retrieve valid current generation_id for schema {self.schema_id}. Saving prompts without generation_id.")
                 prompts.pop("generation_id", None) # Ensure it's not stale

            # Save the prompts to file
            try:
                PROMPT_DIR.mkdir(parents=True, exist_ok=True) # Ensure directory exists
                with open(prompt_file, "w") as f:
                    json.dump(prompts, f, indent=2)
                print(f"Prompts saved to {prompt_file}")

                QUERY_DIR.mkdir(parents=True, exist_ok=True) # Ensure directory exists
                with open(query_file, "w") as f:
                    # Format the sample queries to match what the API expects
                    # The API expects a dictionary with categories as keys and lists of queries as values
                    formatted_queries = {
                        "general": [],
                        "relationships": [],
                        "domain": []
                    }
                    
                    # Add each sample query to an appropriate category based on content analysis
                    for i, query in enumerate(prompts["sample_queries"]):
                        query_lower = query.lower()
                        
                        # Determine the best category based on query content
                        if any(keyword in query_lower for keyword in ["relation", "connect", "link", "between", "path"]):
                            category = "relationships"
                        elif any(keyword in query_lower for keyword in ["domain", "specific", "industry", "field", "area"]):
                            category = "domain"
                        else:
                            category = "general"  # Default category
                        
                        formatted_queries[category].append({
                            "id": f"{category}_{len(formatted_queries[category])+1}",
                            "query": query,
                            "description": f"AI-generated sample query #{i+1}"
                        })
                    
                    json.dump(formatted_queries, f, indent=2)
                print(f"Queries saved to {query_file}")

            except Exception as e:
                print(f"Error saving prompts to {prompt_file}: {e}")
                print("Warning: Prompts generated but failed to save to file. Using in-memory prompts for this session.")
        
    def _generate_prompts(self) -> Dict[str, Any]:
        """Generate custom prompts using LLM based on the schema"""
        
        # Define a system prompt for prompt generation
        cypher_system_prompt = """
        You are a Knowledge Graph expert tasked with creating optimal prompts for Neo4j graph database interactions. 
        You're being given a Neo4j graph schema in JSON format.
        
        Your job is to analyze the schema and generate a Cypher query generation prompt that will help an LLM convert natural language questions to Cypher queries
        1. Do not include any additional text other than the prompt template in required output format
        2. Do not include duplicate output sections
        3. The prompts should be tailored to the specific domain and structure of the knowledge graph as defined in the schema.
        """

        # Define a system prompt for prompt generation
        qa_system_prompt = """
        You are a Knowledge Graph expert tasked with creating optimal prompts for Neo4j graph database interactions. 
        You're being given a Neo4j graph schema in JSON format.
        
        Your job is to analyze the schema and generate a QA prompt that will help an LLM interpret Cypher query results and answer user questions
        1. Do not include any additional text other than the prompt template in required output format
        2. Do not include duplicate output sections
        3. The prompts should be tailored to the specific domain and structure of the knowledge graph as defined in the schema.
        """
        
        sample_queries_system_prompt = """
        You are a Knowledge Graph expert tasked with creating optimal prompts for Neo4j graph database interactions. 
        You're being given a Neo4j graph schema in JSON format.
        
        Your job is to analyze the schema, given data and generate a sample queries generation prompt that will help an LLM generate sample queries based on the graph schema
        1. Do not include any additional text other than the prompt template in required output format
        2. Do not include duplicate output sections
        3. The prompts should be tailored to the specific domain and structure of the knowledge graph as defined in the schema.
        """

        # Define the prompt for generating the Cypher generation prompt
        # IMPORTANT: Updated to use 'query' instead of 'question' for compatibility with GraphCypherQAChain
        cypher_prompt_instruction = """
        Create a prompt template for generating Neo4j Cypher queries.
        
        The template should:
        1. Be tailored to the specific domain and structure of this knowledge graph
        2. Include specific node labels, relationship types, and properties from the schema
        3. Provide guidance on Neo4j Cypher best practices
        4. Include examples of good question patterns based on this schema
        6. The output must ONLY contain the executable Cypher query string without quotes or backticks around it
        7. VERY IMPORTANT: While generating Cypher queries, embed values directly. DO NOT use parameters like $name.
           - For string values, use single quotes: `{{name: 'Example Name'}}` (Note the double braces for the example itself!)
           - For numeric values, use them directly: `{{born: 1234}}` (Note the double braces!)
           - Ensure example Cypher queries you provide in the prompt follow this pattern, e.g.: `MATCH (p:Person {{name: 'Example Name'}})-[:ACTED_IN]->(m:Movie) RETURN m.title`
        
        8. Include example queries **Examples** section as per output format, use the following format to make it clear what is the natural language and what is the Cypher code:
           DO NOT put {question} in the example queries
            Example 1: <example_natural_language_query>
            cypher: MATCH (n) RETURN n LIMIT 5
            
            Example 2: <example_natural_language_query>
            cypher: MATCH (p)-[r]->(m) RETURN p, r, m LIMIT 5
        
        9. Include placeholders for {question} where the user question will be inserted
        
        10. Do not use generic examples - use the actual node labels, relationship types and properties from the provided schema.
        Keep the prompt concise but comprehensive enough to guide accurate Cypher query generation.
        
        11. IMPORTANT: Use {question} (not {{question}} or query) as the placeholder for the user's question, as this is required for compatibility with the GraphCypherQAChain.
        12. Append following critical instructions to **Instructions for Cypher Query Generation:** section:
            - Cypher query should only include names of node, relationship or properties as per given schema
            - Return ONLY a single raw Cypher query with NO explanations, NO headers, NO numbering, NO query repetition, NO backticks, and NO additional text of any kind
            - The response should be ONLY the executable Cypher query with no additional text. The LLM must not include any of the following in its response:
                - NO "Simple Query:" or "Medium Query:" or "Complex Query:" headers
                - NO numbered lists like "1." or "2."
                - NO explanations before or after the query
                - NO backticks around the query
                - NO examples of multiple queries - just one single executable query
                - NO "Here's a Cypher query that..." text
                - NO "This query will..." explanatory text
            - If no cypher query could be generated for given query then return None or empty. DO NOT RETURN text with explaination.
        13. Do not alter **Sample Cyphers used for node/relationship creation** section
        14. Output Format:
        **Your Role:**
        **Your Task:**
        **Schema:**
          **Nodes:**
          **Relationships:**
        **Sample Cyphers used for node/relationship creation:**
        {sample_cyphers}
        **Cypher Best Practices:**
        **Value Embedding:**
        **Examples:**
        **Instructions for Cypher Query Generation:**
        **Question:**
        """
        
        # Define the prompt for generating the QA prompt
        # IMPORTANT: Updated to use simple {question} and {response} for compatibility with GraphCypherQAChain
        qa_prompt_instruction = """
        Create a prompt template for answering questions based on Neo4j query results.
        
        The template should:
        1. Guide the response generation for questions about this specific knowledge graph
        2. Include domain-specific guidance based on the node types and relationships in the schema
        3. Provide instructions on how to interpret and present the query results
        4. IMPORTANT: Include placeholders for {question} and {context} 
        5. Suggest how to handle common scenarios like empty results or large result sets
        6. Make the prompt specific to this graph's domain and structure, not generic.
        7. Include specific examples based on schema only, examples should similar in format to following -
            **Example 1:**
                *   `question`: "How many senior citizens have churned?"
                *   `context`: `[{{"count": 150}}]`
                *   *Answer:* "There are 150 senior citizens who have churned."

            **Example 2:**
                *   `question`: "What are the details for customer '1234-ABCD'?"
                *   `context`: `[{{"customerID": "1234-ABCD", "tenure": 24, "Contract": "Month-to-month", "MonthlyCharges": 75.50, "Churn": "No"}}]`
                *   *Answer:* "Customer '1234-ABCD' has been with us for 24 months, is on a Month-to-month contract, has monthly charges of $75.50, and has not churned."

            **Example 3:**
                *   `question`: "Are there any customers with Fiber optic who pay by Electronic check?"
                *   `context`: `[]`
                *   *Answer:* "No, I could not find any customers who have Fiber optic internet service and pay by Electronic check."

        8. Append following critical instructions to **Instructions for Interpretation and Response:** section. Do not include {question} or {context} directly in **Instructions for Interpretation and Response:** section instead if needed just use words 'question', 'context'
            - Return ONLY the answer string with data from context or question
            - Return "It seems like there is no data to support this query" string if no answer can be generated
            - Use data only from context or question to prepare answer
        9. Output Format:
        **Your Role:**
        **Your Task:**
        **Schema:**
          **Nodes:**
          **Relationships:**
        **Cypher Results Context:**
        **User Question:**
        **Example Scenario:**
        **Instructions for Interpretation and Response:**
        """
        
        # Define the prompt for generating sample queries
        sample_queries_instruction = """
        Generate 15 sample natural language questions based on provided knowledge graph schema and data from original dataset
        
        The questions should:
        1. Reflect the actual structure and domain of this knowledge graph
        2. Use the real node labels, relationship types and properties from the schema
        3. Include a mix of simple and complex queries
        4. Focus on questions that would provide meaningful insights
        5. Be organized as a JSON array of strings
        6. Generate questions using sample data values only
        
        Return ONLY the JSON array of sample questions, nothing else.
        """
        
        try:
            # Generate Cypher prompt template
            print(f"Generating Cypher prompt template for {self.db_id}...")
            cypher_messages = [
                {"role": "system", "content": cypher_system_prompt},
                {"role": "user", "content": f"Use this Neo4j schema: {self.formatted_schema} for generating Cypher prompt as per following instructions:\n\n{cypher_prompt_instruction}"}
            ]
            llm_local = LLMProvider.get_llm(provider_name=LLMConstants.Providers.GOOGLE, model_name=LLMConstants.GoogleModels.DEFAULT, temperature=0.0)
            cypher_response = llm_local.invoke(cypher_messages)
            cypher_prompt_template = cypher_response.content.strip()
            
            cypher_generator = CsvToCypherGenerator(self.schema, self.csv_file_path, LLMConstants.Providers.GOOGLE, LLMConstants.GoogleModels.DEFAULT)
            cypher_queries = cypher_generator.generate_cypher_for_rows()
            #cypher_queries_str = '\n'.join(cypher_queries)
            #import_notes = '\n\n Note: DO NOT return example question and cypher query. Return PROPER cypher query only. DO NOT include any explanation'
            #cypher_prompt_template = f"### Task: Generate Cypher queries for time-series data using this schema:\n\n### Cypher Queries:\n\n{cypher_queries_str}\n\n{import_notes}\n\n{cypher_prompt_template}"

            # Apply Neo4j property syntax escaping to prevent template variable confusion
            cypher_prompt_template = self._escape_neo4j_properties(cypher_prompt_template)
            cypher_prompt_template = cypher_prompt_template.replace("{context}", "")

            
            # Generate QA prompt template
            print(f"Generating QA prompt template for {self.db_id}...")
            qa_messages = [
                {"role": "system", "content": qa_system_prompt},
                {"role": "user", "content": f"Use this Neo4j schema: {self.formatted_schema} for generating QA prompt as per following instructions:\n\n{qa_prompt_instruction}"}
            ]
            qa_response = llm_local.invoke(qa_messages)
            qa_prompt_template = qa_response.content.strip()
            
            # Apply Neo4j property syntax escaping to prevent template variable confusion
            qa_prompt_template = self._escape_neo4j_properties(qa_prompt_template)
            # Append formatted schema at the front of the template
            #qa_prompt_gen_txt = 'You are an expert at answering questions using data from a knowledge graph. You will receive a question and the results of a Cypher query executed against the graph. Your task is to interpret the Cypher query results and provide a concise and informative natural language answer to the original question.'
            #qa_prompt_template = f"{qa_prompt_gen_txt}\n\n{self.formatted_schema}\n\n{qa_prompt_template}"
            
            # Generate sample data JSON
            sample_data = self._generate_sample_data_json()
            print(f"Sample data JSON for Queries!!!: {sample_data}")
            # Generate sample queries
            sample_queries_messages = [
                {"role": "system", "content": sample_queries_system_prompt},
                {"role": "user", "content": f"Use this Neo4j schema: {self.formatted_schema} and the data to use: {sample_data} for generating sample questions prompt as per following instruction: \n\n{sample_queries_instruction}"}
            ]
            sample_queries_response = llm_local.invoke(sample_queries_messages)
            
            # Extract JSON array from response
            try:
                # Attempt to parse the response as JSON
                sample_queries_text = sample_queries_response.content.strip()
                # Extract JSON array if it's wrapped in markdown code blocks or additional text
                if "```json" in sample_queries_text and "```" in sample_queries_text:
                    # Extract content between ```json and ```
                    json_content = sample_queries_text.split("```json")[1].split("```")[0].strip()
                    sample_queries = json.loads(json_content)
                elif "[" in sample_queries_text and "]" in sample_queries_text:
                    # Find the first [ and last ] in the text
                    start_idx = sample_queries_text.find("[")
                    end_idx = sample_queries_text.rfind("]")+1
                    json_content = sample_queries_text[start_idx:end_idx]
                    sample_queries = json.loads(json_content)
                else:
                    # Try parsing the whole response
                    sample_queries = json.loads(sample_queries_text)
            except Exception as e:
                print(f"Error parsing sample queries JSON: {e}")
                # Fallback to default sample queries
                sample_queries = [
                    "What are the main entities in this knowledge graph?",
                    "How many nodes and relationships are in the graph?",
                    "What is the overall structure of this knowledge graph?"
                ]
                            
            # Create prompts dictionary
            prompts = {
                "db_id": self.db_id,
                "cypher_prompt": cypher_prompt_template,
                "qa_prompt": qa_prompt_template,
                "sample_queries": sample_queries,
                "sample_cyphers": cypher_queries
            }
            print(f"DEBUG: Created prompts with properly escaped Neo4j property syntax")
            
            return prompts
            
        except Exception as e:
            print(f"Error generating prompts with LLM: {e}")
            import traceback
            print(f"Traceback: {traceback.format_exc()}")
            
            # Instead of using fallback prompts, raise the exception to fail explicitly
            # This makes debugging easier by exposing the actual error
            raise RuntimeError(f"Failed to generate prompts for schema: {str(e)}")
        
    def _generate_sample_data_json(self):
        """Generate a JSON with column names as keys and lists of unique values for each column."""
        import pandas as pd
        import json
        
        try:
            # Read only the first 100 rows of the CSV to avoid memory issues with large files
            df = pd.read_csv(self.csv_file_path, nrows=100)
            
            # Create a dictionary to store column names and unique values
            sample_data = {}
            
            # For each column, get up to 10 unique values
            for column in df.columns:
                unique_values = df[column].dropna().unique()
                # Take only up to 10 unique values
                sample_values = unique_values[:10].tolist()
                
                # Convert numpy types to native Python types for JSON serialization
                sample_values = [
                    v.item() if hasattr(v, 'item') else v 
                    for v in sample_values
                ]
                
                sample_data[column] = sample_values
            
            return json.dumps(sample_data, indent=2)
        except Exception as e:
            print(f"Error generating sample data JSON: {str(e)}")
            return "{}"

    def _escape_neo4j_properties(self, prompt_text: str) -> str:
        """
        Escapes Neo4j property maps (e.g., {key: value}) within node/relationship patterns (...) or [...]
        by wrapping them in double curly braces {{ {key: value} }} for LangChain compatibility.
        It avoids wrapping already escaped maps {{...}} and template variables like {question} or {context}.
        """
        if not prompt_text:
            return prompt_text

        # First, temporarily replace {question} and {context} with placeholders
        # to prevent them from being double-escaped
        placeholder_map = {
            "{question}": "__QUESTION_PLACEHOLDER__",
            "{context}": "__CONTEXT_PLACEHOLDER__",
            "{query}": "__QUERY_PLACEHOLDER__"
        }
        
        # Replace template variables with placeholders
        for template_var, placeholder in placeholder_map.items():
            prompt_text = prompt_text.replace(template_var, placeholder)
        
        # Now escape all remaining curly braces
        prompt_text = prompt_text.replace("{", "{{").replace("}", "}}")
        
        # Restore the template variables from placeholders
        for template_var, placeholder in placeholder_map.items():
            prompt_text = prompt_text.replace(placeholder, template_var)
            
        return prompt_text

    def _load_prompts(self) -> None:
        """Load custom prompts for this source"""
        # Use both db_id and schema_id in the prompt filename for better specificity
        prompt_file = PROMPT_DIR / f"prompt_{self.db_id}_{self.schema_id}.json"
        
        try:
            # Default to None initially to detect loading issues
            self.cypher_prompt = None
            self.qa_prompt = None
            
            print(f"DEBUG: Attempting to load prompts")
            prompts = {}
            
            # Try to load the specific prompt file first
            if prompt_file.exists():
                with open(prompt_file, "r") as f:
                    prompts = json.load(f)
                print(f"DEBUG: Loaded prompt file for db '{self.db_id}' and schema '{self.schema_id}'")
            else:
                print(f"DEBUG: No prompt files found, will use default prompts")
                
            # Convert string prompts to ChatPromptTemplate objects if needed
            if "cypher_prompt" in prompts:
                cypher_prompt_text = prompts.get("cypher_prompt")
                # Check if it's already a structured object or a string
                if isinstance(cypher_prompt_text, str):
                    print(f"DEBUG: Converting cypher_prompt string to ChatPromptTemplate")
                    # Check if the prompt starts with "Prompt:" or "Prompt Template:" and clean it
                    if cypher_prompt_text.startswith("Prompt:") or cypher_prompt_text.startswith("Prompt Template:"):
                        lines = cypher_prompt_text.split('\n')
                        # Skip the first line if it's just the "Prompt:" header
                        template_text = '\n'.join(lines[1:]).strip()
                        print(f"DEBUG: Cleaned prompt template from header")
                    else:
                        template_text = cypher_prompt_text
                        
                    # Fix all placeholder formats for compatibility with GraphCypherQAChain
                    # The key parameter expected by the chain is 'query', not 'question'
                    # if "{{query}}" in template_text:
                    #     template_text = template_text.replace("{{query}}", "{query}")
                    #     print(f"DEBUG: Fixed {{query}} placeholder format")
                    
                    # if "{{question}}" in template_text:
                    #     # Replace 'question' with 'query' for compatibility
                    #     template_text = template_text.replace("{{question}}", "{query}")
                    #     print(f"DEBUG: Replaced {{question}} with {{query}} for compatibility")
                    
                    self.cypher_prompt = ChatPromptTemplate.from_template(template_text)
                else:
                    self.cypher_prompt = cypher_prompt_text
            
            if "qa_prompt" in prompts:
                qa_prompt_text = prompts.get("qa_prompt")
                # Check if it's already a structured object or a string
                if isinstance(qa_prompt_text, str):
                    print(f"DEBUG: Converting qa_prompt string to ChatPromptTemplate")
                    # Check if the prompt starts with "Prompt:" or "Prompt Template:" and clean it
                    if qa_prompt_text.startswith("Prompt:") or qa_prompt_text.startswith("Prompt Template:"):
                        lines = qa_prompt_text.split('\n')
                        # Skip the first line if it's just the "Prompt:" header
                        template_text = '\n'.join(lines[1:]).strip()
                        print(f"DEBUG: Cleaned prompt template from header")
                    else:
                        template_text = qa_prompt_text
                        
                    # Fix all placeholder formats for compatibility with GraphCypherQAChain
                    # QA prompt should use 'query' and 'context' parameters
                    # LangChain specifically expects 'query' not 'question'
                    # if "{{question}}" in template_text:
                    #     # Always replace 'question' with 'query' - this is critical
                    #     template_text = template_text.replace("{{question}}", "{query}")
                    #     print(f"DEBUG: Replaced {{question}} with {{query}} in QA prompt")
                        
                    # if "{{query}}" in template_text:
                    #     template_text = template_text.replace("{{query}}", "{query}")
                    #     print(f"DEBUG: Fixed {{query}} format in QA prompt")
                        
                    # if "{{context}}" in template_text:
                    #     template_text = template_text.replace("{{context}}", "{context}")
                    #     print(f"DEBUG: Fixed {{context}} format in QA prompt")
                        
                    # if "{{response}}" in template_text:
                    #     # Also replace 'response' with 'context' as needed
                    #     template_text = template_text.replace("{{response}}", "{context}")
                    #     print(f"DEBUG: Replaced {{response}} with {{context}} in QA prompt")
                            
                    print(f"DEBUG: Fixed placeholders in QA prompt")
                    self.qa_prompt = ChatPromptTemplate.from_template(template_text)
                else:
                    self.qa_prompt = qa_prompt_text
                
            # Load sample queries 
            self.sample_queries = prompts.get("sample_queries", [])
            
        except Exception as e:
            # Handle any errors in loading prompts
            print(f"ERROR: Failed to load prompts: {str(e)}")
            print(f"DEBUG: {traceback.format_exc()}")
            
            # Instead of using fallback prompts, raise the exception to fail explicitly
            # This makes debugging easier by exposing the actual error
            raise RuntimeError(f"Failed to load prompts for schema: {str(e)}") from e
            
    def _debug_print_prompts(self):
        """Print debug information about the loaded prompts"""
        print("\n=== DEBUG: PROMPT INFORMATION ===")
        print(f"Cypher prompt type: {type(self.cypher_prompt)}")
        print(f"QA prompt type: {type(self.qa_prompt)}")
        
        # Check prompt template variable names
        if hasattr(self.cypher_prompt, 'template') and hasattr(self.cypher_prompt.template, 'variable_names'):
            print(f"Cypher prompt variables: {self.cypher_prompt.template.variable_names}")
        elif hasattr(self.cypher_prompt, 'input_variables'):
            print(f"Cypher prompt variables: {self.cypher_prompt.input_variables}")
        else:
            print("Cannot determine Cypher prompt variables")
            
        if hasattr(self.qa_prompt, 'template') and hasattr(self.qa_prompt.template, 'variable_names'):
            print(f"QA prompt variables: {self.qa_prompt.template.variable_names}")
        elif hasattr(self.qa_prompt, 'input_variables'):
            print(f"QA prompt variables: {self.qa_prompt.input_variables}")
        else:
            print("Cannot determine QA prompt variables")
        print("===================================\n")

    def _get_cypher_from_table(self, model_class) -> str:
        """Reads loading cypher from the specified table based on schema_id."""
        db = None
        try:
            db = SessionLocal()
            record = db.query(model_class).filter(model_class.schema_id == self.schema_id).first()
            if record and record.cypher:
                print(f"Found loading cypher in {model_class.__tablename__} for schema {self.schema_id}")
                return record.cypher
            else:
                print(f"No loading cypher found in {model_class.__tablename__} for schema {self.schema_id}")
                return "" # Return empty string if not found
        except Exception as e:
            print(f"Error reading from {model_class.__tablename__} for schema {self.schema_id}: {e}")
            if db:
                db.rollback() # Rollback on error, though it's just a read
            return "" # Return empty string on error
        finally:
            if db:
                db.close()

    def _format_schema(self):
        """
        Convert the class's schema property to a formatted text representation
        suitable for LLM prompts.
        
        Returns:
            str: A formatted text representation of the schema listing nodes and relationships
        """
        try:
            # Use the class's schema property
            schema = self.schema

            # Start building the schema text
            schema_text = "The graph contains the following nodes and relationships:\n\nNodes:\n"
            
            # Process nodes
            if 'nodes' in schema:
                for node in schema['nodes']:
                    node_label = node.get('label', 'UnknownNode')
                    schema_text += f"\n  {node_label}:\n"
                    
                    if 'properties' in node and node['properties']:
                        # Handle properties as a dictionary of key:type pairs
                        for prop_name, prop_type in node['properties'].items():
                            # Format property with type
                            schema_text += f"    {prop_name} ({prop_type})\n"
                    else:
                        schema_text += "    (no properties)\n"

            # Process relationships
            schema_text += "\nRelationships:\n"
            
            if 'relationships' in schema:
                for relationship in schema['relationships']:
                    rel_type = relationship.get('type', 'UNKNOWN_RELATIONSHIP')
                    # Handle different formats of relationship node references
                    start_node = None
                    end_node = None
                    
                    # Try different ways the schema might represent source/target nodes
                    if 'startNode' in relationship:
                        start_node = relationship['startNode']
                    elif 'source' in relationship:
                        start_node = relationship['source']
                    
                    if 'endNode' in relationship:
                        end_node = relationship['endNode']
                    elif 'target' in relationship:
                        end_node = relationship['target']
                    
                    schema_text += f"\n  [{start_node}]-[{rel_type}]->({end_node}):\n"
                    
                    if 'properties' in relationship and relationship['properties']:
                        # Handle properties as a dictionary of key:type pairs
                        for prop_name, prop_type in relationship['properties'].items():
                            # Format property with type
                            schema_text += f"    {prop_name} ({prop_type})\n"
                    else:
                        schema_text += "    (no properties)\n"
            
            self.formatted_schema = schema_text
            print(f"DEBUG: Successfully formatted schema")
        
        except Exception as e:
            print(f"ERROR: Failed to convert schema to text: {str(e)}")
            traceback.print_exc()
            # Set a fallback formatted schema
            self.formatted_schema = "Schema information not available in expected format."

    def _format_history(self) -> str:
        """Format last exchanges for context"""
        return "\n".join(
            f"{msg.content}" 
            for msg in self.history.messages[-5:] if msg.type == "ai"
        )

    @cacheable()
    def _extract_valid_cypher_query(self, llm_output: str) -> str:
        """
        Extract a valid Cypher query from the LLM's output.
        
        Args:
            llm_output: The raw output from the LLM containing Cypher query
            
        Returns:
            str: A clean, executable Cypher query
        """
        print(f"DEBUG: Extracting valid Cypher query from LLM output")
        print(f"DEBUG: Raw LLM output:\n{llm_output}")
        
        # List of valid Cypher keywords to check for
        valid_keywords = ["MATCH", "RETURN", "CREATE", "MERGE", "WITH", "CALL", "OPTIONAL", "UNWIND"]
        
        # Check if the output contains 'cypher query:' pattern
        try:
            match = re.search(r'cypher query:\s*(.+?)(?:\n|$)', llm_output, re.IGNORECASE)
            if match:
                query = match.group(1).strip()
                print(f"DEBUG: Found query using 'cypher query:' pattern: {query}")
                return query
        except Exception as e:
            print(f"WARNING: Error in 'cypher query:' pattern matching: {str(e)}")
        
        # Look for numbered queries in the output (e.g., "1. MATCH (n) RETURN n")
        try:
            # First, remove sections like "Simple Query:" or "Medium Query:" or "Complex Query:"
            # by splitting the text into sections and processing each section
            sections = re.split(r'(Simple|Medium|Complex)\s+Query:', llm_output)
            
            for i in range(1, len(sections), 2):  # Process each section after a header
                if i+1 < len(sections):
                    section_text = sections[i+1]
                    # Look for numbered queries in this section
                    numbered_matches = re.findall(r'\d+\.\s*(`?)([^`\n]+)(`?)', section_text)
                    for match in numbered_matches:
                        query_text = match[1].strip()
                        # Check if it starts with a valid Cypher keyword
                        if any(query_text.upper().startswith(keyword) for keyword in valid_keywords):
                            print(f"DEBUG: Found numbered query: {query_text}")
                            return query_text
        except Exception as e:
            print(f"WARNING: Error in numbered query extraction: {str(e)}")
        
        # Look for content within backticks
        try:
            matches = re.findall(r'`([^`]+)`', llm_output)
            if matches:
                # Find the first match that starts with a valid Cypher keyword
                for match in matches:
                    cleaned = match.strip()
                    if any(cleaned.upper().startswith(keyword) for keyword in valid_keywords):
                        print(f"DEBUG: Found query in backticks: {cleaned}")
                        return cleaned
        except Exception as e:
            print(f"WARNING: Error in backtick pattern matching: {str(e)}")
        
        # If we can't find a pattern, try to extract a valid Cypher query based on keywords
        try:
            lines = llm_output.split('\n')
            for line in lines:
                cleaned = line.strip()
                if any(cleaned.upper().startswith(keyword) for keyword in valid_keywords):
                    print(f"DEBUG: Found query by keyword: {cleaned}")
                    return cleaned
        except Exception as e:
            print(f"WARNING: Error in keyword extraction: {str(e)}")
        
        # If all else fails, return a simple query
        print(f"WARNING: Could not extract a valid Cypher query from LLM output. Using fallback query.")
        return "MATCH (n) RETURN labels(n) as labels, count(n) as count LIMIT 10"

    def _safe_execute_query(self, graph, query):
        """Safely execute a Neo4j query without risking recursion"""
        try:
            # Use the original query method directly
            original_query = graph.__class__.query
            return original_query(graph, query)
        except Exception as e:
            print(f"ERROR: Query execution failed: {str(e)}")
            return [{"error": f"Query failed: {str(e)}"}]

    def query(self, question: str, cypher_queries: str) -> Dict[str, Any]:
        """Process user queries against the knowledge graph"""
        try:
            print(f"\n===== DEBUG: PROCESSING QUERY: '{question}' =====")
            # Add detailed diagnostic info about prompts
            self._debug_print_prompts()
            
            # Execute chain with context
            hist = self._format_history()
            print(f"HISTORY TO BE USED: {hist}")
            
            # Add schema awareness to the query
            start_time = time.time()
            
            # Handle schema-related queries directly
            if any(keyword in question.lower() for keyword in ['schema', 'structure', 'entities', 'nodes', 'relationships']):
                try:
                    # Get node information
                    node_query = """
                    MATCH (n)
                    WITH DISTINCT labels(n) as labels, count(n) as count
                    RETURN {labels: labels, count: count} as nodeInfo
                    """
                    node_info = self._safe_execute_query(self.chain.graph, node_query)
                    
                    # Get relationship information
                    rel_query = """
                    MATCH ()-[r]->() 
                    WITH DISTINCT type(r) as type, count(r) as count
                    RETURN {type: type, count: count} as relInfo
                    """
                    rel_info = self._safe_execute_query(self.chain.graph, rel_query)
                    
                    # Format results nicely
                    nodes_str = "\nNodes:\n" + "\n".join([f"- {info['nodeInfo']['labels']}: {info['nodeInfo']['count']} instances" for info in node_info])
                    rels_str = "\nRelationships:\n" + "\n".join([f"- {info['relInfo']['type']}: {info['relInfo']['count']} instances" for info in rel_info])
                    
                    return {"result": f"Here's the graph structure:{nodes_str}{rels_str}"}

                except Exception as schema_e:
                    print(f"ERROR: Schema query failed: {str(schema_e)}")
                    # Fall through to regular processing
            
            # Use a direct approach to generate and execute Cypher
            try:
                # Step 1: Generate Cypher using the cypher_prompt
                cypher_gen_inputs = {"query": question}
                if hasattr(self.chain, "cypher_llm") and hasattr(self.chain, "cypher_prompt"):
                    print("DEBUG: Directly generating Cypher using the cypher prompt")
                    generated_cypher = self.chain.cypher_llm.invoke(
                        self.chain.cypher_prompt.format(**cypher_gen_inputs)
                    ).content
                    print(f"DEBUG: Generated Cypher:\n{generated_cypher}")
                    # Check if the Cypher query is None or empty
                    if generated_cypher is None or generated_cypher.strip() == "":
                        print("DEBUG: Generated Cypher query is None or empty")
                        return {"result": "Not able to prepare valid queries. Update your queries with appropriate data attributes"}

                    # Step 2: Extract valid Cypher query
                    clean_cypher = self._extract_valid_cypher_query(generated_cypher)


                    try:
                        print(f"DEBUG: Executing extracted Cypher: {generated_cypher}")
                        context = self._safe_execute_query(self.chain.graph, clean_cypher)
                    except Exception as exec_error:
                        print(f"DEBUG: Error executing extracted query: {str(exec_error)}")
                        # Try to extract a different query if the first one fails
                        fallback_cypher = "MATCH (n) RETURN labels(n) as labels, count(n) as count LIMIT 10"
                        print(f"DEBUG: Falling back to simple query: {fallback_cypher}")
                        context = self._safe_execute_query(self.chain.graph, fallback_cypher)
                    
                    # Step 4: Generate the final answer using the qa_prompt
                    qa_inputs = {"query": question, "context": context}
                    if hasattr(self.chain, "qa_llm") and hasattr(self.chain, "qa_prompt"):
                        print("DEBUG: Generating answer using QA prompt")
                        answer = self.chain.qa_llm.invoke(
                            self.chain.qa_prompt.format(**qa_inputs)
                        ).content
                        result = {"result": answer}
                    else:
                        result = {"result": f"Query results: {context}"}
                else:
                    # Fallback to standard chain invocation
                    print("DEBUG: Falling back to standard chain invocation")
                    try:
                        result = self.chain.invoke({'question': question, 'query': question, 'sample_cyphers': cypher_queries})
                    except Exception as chain_error:
                        print(f"DEBUG: Standard chain invocation failed: {str(chain_error)}")
                        # Check if error is related to None Cypher query
                        if "Invalid input 'None'" in str(chain_error):
                            return {"result": "Not able to prepare valid queries. Please update your question with specific data attributes or relationships."}
                        # Otherwise, raise the error to be caught by the outer try-except
                        raise
            except Exception as e:
                print(f"ERROR: Direct approach failed: {str(e)}")
                print(f"DEBUG: {traceback.format_exc()}")
                
                # Fallback to a simple query if everything else fails
                try:
                    print("DEBUG: Falling back to simple query")
                    fallback_query = "MATCH (n) RETURN labels(n) as labels, count(n) as count LIMIT 10"
                    context = self._safe_execute_query(self.chain.graph, fallback_query)
                    result = {
                        "result": f"I encountered an error processing your query. \n\nHere's some basic information about the graph: {context}"
                    }
                except Exception as e2:
                    print(f"ERROR: Even simple fallback failed: {str(e2)}")
                    result = {
                        "result": f"I encountered multiple errors processing your query. The database might be unavailable or the query was malformed."
                    }
            
            query_time = time.time() - start_time
            print(f"Query completed in {query_time:.2f} seconds")
            
            # Persist conversation
            print(f"Persisting conversation: {result}")
            self.history.add_user_message(question)
            self.history.add_ai_message(result["result"])
            
            return result
        
        except Exception as e:
            print(f"Research error: {str(e)}")
            print(f"Full error traceback: {traceback.format_exc()}")
            return {"result": f"An error occurred while processing your query: {str(e)}. Please try again or contact support."}

    def __del__(self):
        # Close the cache connection when the object is garbage collected
        if hasattr(self, 'cache'):
            self.cache.close()

# Singleton-like behavior with dict of assistants by db_id
_assistants = {}
_lock = threading.Lock()

def get_schema_aware_assistant(db_id: str, schema_id: str, schema: str, session_id: str = None) -> SchemaAwareGraphAssistant:
    """
    Get a schema-aware assistant for the specified database ID.
    Creates a new assistant if one doesn't exist, otherwise returns the existing one.
    
    Args:
        db_id: ID of the Neo4j database to query
        schema_id: ID of the schema being used
        schema: JSON string containing the schema
        session_id: Optional session ID for chat history
    Returns:
        SchemaAwareGraphAssistant: The assistant for the database
    """
    # Create a unique key combining db_id, schema_id and session_id
    key = f"{db_id}:{schema_id}:{session_id}" if session_id else f"{db_id}:{schema_id}"
    
    try:
        with _lock:
            if key not in _assistants:
                # Log connection attempt for debugging
                print(f"DEBUG: Creating new schema-aware assistant for {db_id} with schema {schema_id}")
                print(f"DEBUG: Schema: {schema}")
                _assistants[key] = SchemaAwareGraphAssistant(db_id, schema_id, schema, session_id)
                print(f"DEBUG: Successfully created assistant for {db_id}")
            return _assistants[key]
    except Exception as e:
        # Log the error with detailed information
        error_message = f"Error creating schema-aware assistant for {db_id}: {str(e)}"
        print(f"ERROR: {error_message}")
        
        # Check for specific error types to provide better feedback
        if "Could not use APOC procedures" in str(e):
            print("DEBUG: APOC plugin issue detected. Ensure APOC is installed and configured.")
        elif "authentication failed" in str(e).lower() or "unauthorized" in str(e).lower():
            print("DEBUG: Authentication failed. Check Neo4j credentials.")
        elif "connection refused" in str(e).lower() or "unreachable" in str(e).lower():
            print("DEBUG: Connection failed. Check Neo4j server availability and network settings.")
            
        # Re-raise with a more informative message
        raise RuntimeError(f"Failed to initialize schema-aware assistant: {error_message}") from e
