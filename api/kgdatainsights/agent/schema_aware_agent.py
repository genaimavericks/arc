import os
import json
import time
import hashlib
import threading
import traceback
import re
from pathlib import Path
from uuid import uuid4
from typing import Dict, List, Optional, Any

import httpx
from fastapi import Depends, HTTPException
from dotenv import load_dotenv
from langchain.chat_models import ChatOpenAI
from langchain.chains import GraphCypherQAChain
from langchain.graphs import Neo4jGraph
from langchain.prompts import ChatPromptTemplate
from langchain_neo4j import (
    Neo4jChatMessageHistory
)

from .templates.foamfactory.cypher_prompt_template import CYPHER_GENERATION_PROMPT
from .templates.foamfactory.qa_prompt_template import QA_PROMPT
from .cache import Cache
from .cache import cacheable

from ...kginsights.database_api import get_database_config, parse_connection_params
from ...models import Schema
from ...database import SessionLocal

# Load environment variables
load_dotenv()

# Output directories
OUTPUT_DIR = Path("runtime-data/output/kgdatainsights")
SCHEMA_DIR = OUTPUT_DIR / "schema"
PROMPT_DIR = OUTPUT_DIR / "prompts"

# Create directories if they don't exist
SCHEMA_DIR.mkdir(parents=True, exist_ok=True)
PROMPT_DIR.mkdir(parents=True, exist_ok=True)

class SchemaAwareGraphAssistant:
    """
    Enhanced Graph Assistant that automatically manages schemas and prompts.
    Extends the functionality of Neo4jGraphChatAssistant by adding schema and prompt management.
    """
    def __init__(self, db_id: str, schema_id: str, schema: str, session_id: str = None):
        """
        Initialize the Schema-Aware Graph Assistant.
        
        Args:
            db_id: The ID of the Neo4j database to query
            schema_id: The ID of the schema being used
            schema: The schema content as a string or dict
            session_id: Optional session ID for chat history, generated if not provided
        """
        self.db_id = db_id
        self.schema_id = schema_id
        self.schema = json.loads(schema) if isinstance(schema, str) else schema
        self.session_id = session_id or f"session_{uuid4()}"
        self.formatted_schema = None
        self._format_schema()
        
        # Ensure we have the prompt files
        self._ensure_prompt()
        
        # Get Neo4j connection parameters from the database API
        self.connection_params = self._get_connection_params()
        print(f"Connection params: {self.connection_params}")
        
        # Initialize Neo4j Graph connection
        self.graph = Neo4jGraph(
            url=self.connection_params.get("uri"),
            username=self.connection_params.get("username"),
            password=self.connection_params.get("password"),
            database=self.connection_params.get("database", "neo4j"),
            enhanced_schema=True,
            refresh_schema=False  # Disable schema refresh to avoid APOC dependency
        )
        
        # Initialize Neo4j-backed chat history
        self.history = Neo4jChatMessageHistory(
            session_id=self.session_id,
            url=self.connection_params.get("uri"),
            username=self.connection_params.get("username"),
            password=self.connection_params.get("password"),
            database=self.connection_params.get("database", "neo4j")
        )
        
        # Load the custom prompts for this source
        self._load_prompts()
        
        # Initialize LLM first
        self.llm = ChatOpenAI(model="gpt-4", temperature=0.2)
        
        # Initialize QA chain with Neo4j optimizations using modular LangChain pattern
        try:
            print(f"DEBUG: Initializing GraphCypherQAChain using langchain_neo4j pattern")
            
            # Create a structured chain configuration dictionary
            chain_config = {
                "llm": self.llm,
                "graph": self.graph,
                "verbose": True,
                "return_intermediate_steps": True,
                "allow_dangerous_requests": True
            }
            
            # Only add prompts if they are properly loaded
            if hasattr(self, 'cypher_prompt') and self.cypher_prompt:
                print('DEBUG: cypher prompt found')
                chain_config["cypher_prompt"] = self.cypher_prompt
            
            # QA prompt requires special handling because it relies on intermediate variables
            # that are generated by the chain itself (query and response)
            if hasattr(self, 'qa_prompt') and self.qa_prompt:
                print('DEBUG: qa prompt found - wrapping to handle intermediate variables')
                
                # We need to modify how the QA prompt is passed to work with GraphCypherQAChain
                # The chain internally generates 'query' and 'response' variables
                from langchain_core.prompts import PromptTemplate
                
                # Get the template string from our ChatPromptTemplate
                if hasattr(self.qa_prompt, 'template') and hasattr(self.qa_prompt.template, 'template'):
                    qa_template_str = self.qa_prompt.template.template
                    print(f"DEBUG: Using template string: {qa_template_str[:50]}...")
                    
                    # Create a proper PromptTemplate that GraphCypherQAChain can work with
                    # Based on the error, make sure we include 'query' as an input variable
                    # The langchain_neo4j GraphCypherQAChain specifically expects 'query' not 'question'
                    qa_prompt_template = PromptTemplate(
                        template=qa_template_str,
                        # Both 'query' and 'question' can refer to the same user input
                        # but the chain uses 'query' internally for the user question and for the Cypher query
                        input_variables=["query", "context"]
                    )
                    #chain_config["qa_prompt"] = qa_prompt_template
                else:
                    print(f"DEBUG: Using original QA prompt - may cause errors")
                    #chain_config["qa_prompt"] = self.qa_prompt
            
            # Initialize with the validated configuration
            self.chain = GraphCypherQAChain.from_llm(**chain_config)
            print(f"DEBUG: Successfully initialized GraphCypherQAChain")
            
        except Exception as e:
            print(f"ERROR: GraphCypherQAChain initialization failed: {str(e)}")
            print(f"DEBUG: Error details: {traceback.format_exc()}")        
        
        # Initialize cache
        self.cache = Cache()
    
    def _get_connection_params(self) -> dict:
        """
        Get connection parameters for the specified database ID from the database API.
        
        Returns:
            dict: Connection parameters including uri, username, password, and database
        """
        try:
            # Get the full database configuration
            config = get_database_config()
            
            # Get the specific graph configuration
            graph_config = config.get(self.db_id, {})
            
            if not graph_config:
                print(f"WARNING: No configuration found for database '{self.db_id}' in neo4j.databases.yaml")
                # Try to use default configuration if available
                graph_config = config.get("default", {})
                if graph_config:
                    print(f"INFO: Using 'default' Neo4j configuration as fallback for '{self.db_id}'")
                else:
                    print(f"ERROR: No fallback configuration found for '{self.db_id}'")
            
            # Parse the connection parameters
            params = parse_connection_params(graph_config)
            
            # Validate and log the parameters
            if not params:
                print(f"ERROR: Failed to parse connection parameters for '{self.db_id}'")
                return {
                    "uri": None,
                    "username": None,
                    "password": None,
                    "database": None
                }
            
            # Log the connection parameters (hide password)
            conn_debug = {
                "uri": params.get("uri"),
                "username": params.get("username"),
                "database": params.get("database"),
                "password": "*****" if params.get("password") else None
            }
            print(f"DEBUG: Neo4j connection parameters for '{self.db_id}': {conn_debug}")
            
            # Validate the essential parameters
            if not params.get("uri"):
                print(f"ERROR: Missing Neo4j URI for database '{self.db_id}'")
            
            return params
            
        except Exception as e:
            print(f"ERROR: Failed to get connection params for {self.db_id}: {str(e)}")
            print(f"DEBUG: Stack trace: {traceback.format_exc()}")
            return {
                "uri": None,
                "username": None,
                "password": None,
                "database": None
            }
        
    def _ensure_prompt(self) -> None:
        """Check if prompts exist, validate generation_id, create and save if needed."""
        prompt_file = PROMPT_DIR / f"prompt_{self.db_id}_{self.schema_id}.json"
        regenerate_prompts = False
        current_generation_id = None
        existing_prompts = None

        # 1. Fetch current generation_id from DB
        db = None # Initialize db to None
        try:
            db = SessionLocal()
            schema_record = db.query(Schema).filter(Schema.schema_id == self.schema_id).first()
            if schema_record:
                current_generation_id = schema_record.generation_id
                if not current_generation_id:
                     print(f"Warning: Schema record found for {self.schema_id}, but generation_id is missing. Will regenerate prompts.")
                     regenerate_prompts = True
            else:
                print(f"Warning: Schema record not found for schema_id {self.schema_id}. Cannot verify prompt generation ID. Will regenerate prompts.")
                regenerate_prompts = True # If schema record is missing, prompts might be invalid.
        except Exception as e:
            print(f"Error fetching schema generation_id: {e}. Proceeding, may regenerate prompts.")
            # If DB access fails, we should probably regenerate prompts
            regenerate_prompts = True
        finally:
            if db:
                db.close()

        # 2. Check prompt file and generation_id if regeneration not already decided
        if not regenerate_prompts and prompt_file.exists():
            try:
                with open(prompt_file, "r") as f:
                    existing_prompts = json.load(f)
                
                file_generation_id = existing_prompts.get("generation_id")

                if not current_generation_id: 
                     # This case should be handled above by setting regenerate_prompts=True
                     print(f"Error: Logic flaw - current_generation_id missing but regeneration wasn't triggered.")
                     regenerate_prompts = True 
                elif file_generation_id == current_generation_id:
                    print(f"Prompt file {prompt_file} found and generation_id matches. Loading prompts.")
                    # Load prompts later, set flag to false
                    regenerate_prompts = False 
                else:
                    print(f"Prompt file {prompt_file} found, but generation_id mismatch (File: {file_generation_id}, DB: {current_generation_id}). Regenerating prompts.")
                    regenerate_prompts = True
            except (json.JSONDecodeError, KeyError, Exception) as e:
                print(f"Error reading or parsing prompt file {prompt_file}: {e}. Regenerating prompts.")
                regenerate_prompts = True
        elif not prompt_file.exists():
             print(f"Prompt file for db '{self.db_id}' and schema '{self.schema_id}' not found. Creating prompts...")
             regenerate_prompts = True

        # 3. Regenerate and save if needed
        if regenerate_prompts:
            print("Generating new prompts...")
            # Generate prompts based on schema
            prompts = self._generate_prompts() # Assumes this returns a dict
            
            # Add the current generation_id (if available and valid)
            if current_generation_id:
                prompts["generation_id"] = current_generation_id
            else:
                 print(f"Warning: Could not retrieve valid current generation_id for schema {self.schema_id}. Saving prompts without generation_id.")
                 prompts.pop("generation_id", None) # Ensure it's not stale

            # Save the prompts to file
            try:
                PROMPT_DIR.mkdir(parents=True, exist_ok=True) # Ensure directory exists
                with open(prompt_file, "w") as f:
                    json.dump(prompts, f, indent=2)
                print(f"Prompts saved to {prompt_file}")

            except Exception as e:
                print(f"Error saving prompts to {prompt_file}: {e}")
                print("Warning: Prompts generated but failed to save to file. Using in-memory prompts for this session.")
        else:
            # This case indicates a potential issue: file didn't exist or failed parsing, but regeneration wasn't triggered
            # Or the file existed, matched, but 'existing_prompts' wasn't populated correctly. Log an error.
            print(f"Error: Failed to load prompts for schema {self.schema_id}. File: {prompt_file}. Regeneration status: {regenerate_prompts}")
            raise RuntimeError(f"Failed to load or generate prompts for schema {self.schema_id}.")

    def _generate_prompts(self) -> Dict[str, Any]:
        """Generate custom prompts using LLM based on the schema"""
        # Initialize an LLM for prompt generation
        prompt_generator_llm = ChatOpenAI(model="gpt-4", temperature=0.2)
        
        # Define a system prompt for prompt generation
        system_prompt = """
        You are a Knowledge Graph expert tasked with creating optimal prompts for Neo4j graph database interactions. 
        You're being given a Neo4j graph schema in JSON format.
        
        Your job is to analyze the schema and generate two specialized prompt templates:
        1. A Cypher query generation prompt that will help an LLM convert natural language questions to Cypher queries
        2. A QA prompt that will help an LLM interpret Cypher query results and answer user questions
        
        These prompts should be tailored to the specific domain and structure of the knowledge graph as defined in the schema.
        """
        
        # Define the prompt for generating the Cypher generation prompt
        # IMPORTANT: Updated to use 'query' instead of 'question' for compatibility with GraphCypherQAChain
        cypher_prompt_instruction = """
        Please create a prompt template for generating Neo4j Cypher queries.
        
        The template should:
        1. Be tailored to the specific domain and structure of this knowledge graph
        2. Include specific node labels, relationship types, and important properties from the schema
        3. Provide guidance on Neo4j Cypher best practices
        4. Include examples of good query patterns based on this schema
        5. CRITICALLY IMPORTANT: The final prompt MUST explicitly instruct the LLM to return ONLY a single raw Cypher query with NO explanations, NO headers, NO numbering, NO question repetition, NO backticks, and NO additional text of any kind
        6. The output must ONLY contain the executable Cypher query string without quotes or backticks around it
        8. VERY IMPORTANT: Generate Cypher queries that embed values directly. DO NOT use parameters like $name.
           - For string values, use single quotes: `{{name: 'Example Name'}}` (Note the double braces for the example itself!)
           - For numeric values, use them directly: `{{born: 1234}}` (Note the double braces!)
           - Ensure example Cypher queries you provide in the prompt follow this pattern, e.g.: `MATCH (p:Person {{name: 'Example Name'}})-[:ACTED_IN]->(m:Movie) RETURN m.title`
        
        9. For example queries, use the following format to make it clear what is the natural language and what is the Cypher code:

        Example 1: "<example_natural_language_query>"
        cypher: MATCH (n) RETURN n LIMIT 5
        
        Example 2: "<example_natural_language_query>"
        cypher: MATCH (p)-[r]->(m) RETURN p, r, m LIMIT 5
        
        10. Include placeholders for {query} where the user question will be inserted
        11. EXTREMELY IMPORTANT: The prompt should emphasize that the response should be ONLY the executable Cypher query with no additional text. The LLM must not include any of the following in its response:
            - NO "Simple Query:" or "Medium Query:" or "Complex Query:" headers
            - NO numbered lists like "1." or "2."
            - NO explanations before or after the query
            - NO backticks around the query
            - NO examples of multiple queries - just one single executable query
            - NO "Here's a Cypher query that..." text
            - NO "This query will..." explanatory text
        12. EXTREMELY IMPORTANT: If no cypher query could be generated for given query then return None or empty. DO NOT RETURN text with explaination.
        
        Do not use generic examples - use the actual node labels, relationship types and properties from the provided schema.
        Keep the prompt concise but comprehensive enough to guide accurate Cypher query generation.
        Focus on the most important entity types and relationships that would be commonly queried.
        
        IMPORTANT: Use {query} (not {{query}} or question) as the placeholder for the user's question, as this is required for compatibility with the GraphCypherQAChain.
        """
        
        # Define the prompt for generating the QA prompt
        # IMPORTANT: Updated to use simple {query} and {response} for compatibility with GraphCypherQAChain
        qa_prompt_instruction = """
        Please create a prompt template for answering questions based on Neo4j query results.
        
        The template should:
        1. Guide the response generation for questions about this specific knowledge graph
        2. Include domain-specific guidance based on the node types and relationships in the schema
        3. Provide instructions on how to interpret and present the query results
        4. Include placeholders for {query} and {context} 
        5. Use double curly braces({{{{) to escape single curly braces({{) except for {query} and {context} 
        6. Suggest how to handle common scenarios like empty results or large result sets
        
        Make the prompt specific to this graph's domain and structure, not generic.
        Include specifics about the most important node types and relationships in this particular graph.
        
        IMPORTANT: Use {query} (not {{query}}) for the user's question and generated Cypher query, and {context} (not {{context}}) for the query results, as these exact variable names are required for compatibility with the GraphCypherQAChain.
        """
        
        # Define the prompt for generating sample queries
        sample_queries_instruction = """
        Please generate 10-15 sample natural language questions that would be useful and insightful for this specific knowledge graph.
        
        The questions should:
        1. Reflect the actual structure and domain of this knowledge graph
        2. Use the real node labels, relationship types and properties from the schema
        3. Include a mix of simple and complex queries
        4. Focus on questions that would provide meaningful insights
        5. Be organized as a JSON array of strings
        
        Return ONLY the JSON array of sample questions, nothing else.
        """
        
        try:
            # Generate Cypher prompt template
            print(f"Generating Cypher prompt template for {self.db_id}...")
            cypher_messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Here is the Neo4j schema: {self.formatted_schema}\n\n{cypher_prompt_instruction}"}
            ]
            cypher_response = prompt_generator_llm.invoke(cypher_messages)
            cypher_prompt_template = cypher_response.content.strip()
            
            # Apply Neo4j property syntax escaping to prevent template variable confusion
            cypher_prompt_template = self._escape_neo4j_properties(cypher_prompt_template)
            # Append formatted schema at the front of the template
            cypher_prompt_template = f"{self.formatted_schema}\n\n{cypher_prompt_template}"
            print(f"DEBUG: Applied Neo4j property escaping to Cypher prompt template and appended formatted schema")
            
            # Generate QA prompt template
            print(f"Generating QA prompt template for {self.db_id}...")
            qa_messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Here is the Neo4j schema: {self.formatted_schema}\n\n{qa_prompt_instruction}"}
            ]
            qa_response = prompt_generator_llm.invoke(qa_messages)
            qa_prompt_template = qa_response.content.strip()
            
            # Apply Neo4j property syntax escaping to prevent template variable confusion
            qa_prompt_template = self._escape_neo4j_properties(qa_prompt_template)
            # Append formatted schema at the front of the template
            qa_prompt_template = f"{self.formatted_schema}\n\n{qa_prompt_template}"
            print(f"DEBUG: Applied Neo4j property escaping to QA prompt template and appended formatted schema")
            
            # Generate sample queries
            print(f"Generating sample queries for {self.db_id}...")
            sample_queries_messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Here is the Neo4j schema: {self.formatted_schema}\n\n{sample_queries_instruction}"}
            ]
            sample_queries_response = prompt_generator_llm.invoke(sample_queries_messages)
            
            # Extract JSON array from response
            try:
                # Attempt to parse the response as JSON
                sample_queries_text = sample_queries_response.content.strip()
                # Extract JSON array if it's wrapped in markdown code blocks or additional text
                if "```json" in sample_queries_text and "```" in sample_queries_text:
                    # Extract content between ```json and ```
                    json_content = sample_queries_text.split("```json")[1].split("```")[0].strip()
                    sample_queries = json.loads(json_content)
                elif "[" in sample_queries_text and "]" in sample_queries_text:
                    # Find the first [ and last ] in the text
                    start_idx = sample_queries_text.find("[")
                    end_idx = sample_queries_text.rfind("]")+1
                    json_content = sample_queries_text[start_idx:end_idx]
                    sample_queries = json.loads(json_content)
                else:
                    # Try parsing the whole response
                    sample_queries = json.loads(sample_queries_text)
            except Exception as e:
                print(f"Error parsing sample queries JSON: {e}")
                # Fallback to default sample queries
                sample_queries = [
                    "What are the main entities in this knowledge graph?",
                    "How many nodes and relationships are in the graph?",
                    "What is the overall structure of this knowledge graph?"
                ]
                            
            # Create prompts dictionary
            prompts = {
                "db_id": self.db_id,
                "cypher_prompt": cypher_prompt_template,
                "qa_prompt": qa_prompt_template,
                "sample_queries": sample_queries
            }
            print(f"DEBUG: Created prompts with properly escaped Neo4j property syntax")
            
            return prompts
            
        except Exception as e:
            print(f"Error generating prompts with LLM: {e}")
            import traceback
            print(f"Traceback: {traceback.format_exc()}")
            
            # Instead of using fallback prompts, raise the exception to fail explicitly
            # This makes debugging easier by exposing the actual error
            raise RuntimeError(f"Failed to generate prompts for schema: {str(e)}")
        

    def _escape_neo4j_properties(self, prompt_text):
        """Escape Neo4j property syntax in prompts to prevent template variable confusion
        
        This ensures that expressions like {name: 'John'} are properly escaped as {{name: 'John'}}
        while preserving actual template variables like {query}
        """
        if not prompt_text:
            return prompt_text

        # Check if there are already double braces around property patterns
        # This regex looks for {{prop: value}} patterns which are already escaped
        already_escaped_pattern = re.compile(r'\{\{([a-zA-Z0-9_]+\s*:(?![}]).*?)\}\}')
        
        # Mark positions that are already escaped to avoid double-escaping
        already_escaped_positions = set()
        for match in already_escaped_pattern.finditer(prompt_text):
            start, end = match.span()
            for i in range(start, end):
                already_escaped_positions.add(i)
        
        # Define patterns that need escaping - Neo4j property patterns but not template variables
        # This regex looks for {prop: value} patterns but ignores {query} or {context}
        neo4j_prop_pattern = re.compile(r'\{([a-zA-Z0-9_]+\s*:(?![}]).*?)\}')

        # Find all Neo4j property patterns
        matches = list(neo4j_prop_pattern.finditer(prompt_text))

        # Process matches from end to beginning to avoid offset issues
        for match in reversed(matches):
            start, end = match.span()
            
            # Skip if any part of this match is already in an escaped section
            if any(i in already_escaped_positions for i in range(start, end)):
                continue
            
            # Skip if it looks like a template variable
            content = match.group(1)
            if content.strip() in ["query", "context", "response", "question"]:
                continue

            # Replace with double braces
            prompt_text = prompt_text[:start] + "{" + prompt_text[start:end] + "}" + prompt_text[end:]

        return prompt_text

    
    def _load_prompts(self) -> None:
        """Load custom prompts for this source"""
        # Use both db_id and schema_id in the prompt filename for better specificity
        prompt_file = PROMPT_DIR / f"prompt_{self.db_id}_{self.schema_id}.json"
        
        try:
            # Default to None initially to detect loading issues
            self.cypher_prompt = None
            self.qa_prompt = None
            
            print(f"DEBUG: Attempting to load prompts")
            prompts = {}
            
            # Try to load the specific prompt file first
            if prompt_file.exists():
                with open(prompt_file, "r") as f:
                    prompts = json.load(f)
                print(f"DEBUG: Loaded prompt file for db '{self.db_id}' and schema '{self.schema_id}'")
            else:
                print(f"DEBUG: No prompt files found, will use default prompts")
                
            # Convert string prompts to ChatPromptTemplate objects if needed
            if "cypher_prompt" in prompts:
                cypher_prompt_text = prompts.get("cypher_prompt")
                # Check if it's already a structured object or a string
                if isinstance(cypher_prompt_text, str):
                    print(f"DEBUG: Converting cypher_prompt string to ChatPromptTemplate")
                    # Check if the prompt starts with "Prompt:" or "Prompt Template:" and clean it
                    if cypher_prompt_text.startswith("Prompt:") or cypher_prompt_text.startswith("Prompt Template:"):
                        lines = cypher_prompt_text.split('\n')
                        # Skip the first line if it's just the "Prompt:" header
                        template_text = '\n'.join(lines[1:]).strip()
                        print(f"DEBUG: Cleaned prompt template from header")
                    else:
                        template_text = cypher_prompt_text
                        
                    # Fix all placeholder formats for compatibility with GraphCypherQAChain
                    # The key parameter expected by the chain is 'query', not 'question'
                    if "{{query}}" in template_text:
                        template_text = template_text.replace("{{query}}", "{query}")
                        print(f"DEBUG: Fixed {{query}} placeholder format")
                    
                    if "{{question}}" in template_text:
                        # Replace 'question' with 'query' for compatibility
                        template_text = template_text.replace("{{question}}", "{query}")
                        print(f"DEBUG: Replaced {{question}} with {{query}} for compatibility")
                    
                    self.cypher_prompt = ChatPromptTemplate.from_template(template_text)
                else:
                    self.cypher_prompt = cypher_prompt_text
            
            if "qa_prompt" in prompts:
                qa_prompt_text = prompts.get("qa_prompt")
                # Check if it's already a structured object or a string
                if isinstance(qa_prompt_text, str):
                    print(f"DEBUG: Converting qa_prompt string to ChatPromptTemplate")
                    # Check if the prompt starts with "Prompt:" or "Prompt Template:" and clean it
                    if qa_prompt_text.startswith("Prompt:") or qa_prompt_text.startswith("Prompt Template:"):
                        lines = qa_prompt_text.split('\n')
                        # Skip the first line if it's just the "Prompt:" header
                        template_text = '\n'.join(lines[1:]).strip()
                        print(f"DEBUG: Cleaned prompt template from header")
                    else:
                        template_text = qa_prompt_text
                        
                    # Fix all placeholder formats for compatibility with GraphCypherQAChain
                    # QA prompt should use 'query' and 'context' parameters
                    # LangChain specifically expects 'query' not 'question'
                    if "{{question}}" in template_text:
                        # Always replace 'question' with 'query' - this is critical
                        template_text = template_text.replace("{{question}}", "{query}")
                        print(f"DEBUG: Replaced {{question}} with {{query}} in QA prompt")
                        
                    if "{{query}}" in template_text:
                        template_text = template_text.replace("{{query}}", "{query}")
                        print(f"DEBUG: Fixed {{query}} format in QA prompt")
                        
                    if "{{context}}" in template_text:
                        template_text = template_text.replace("{{context}}", "{context}")
                        print(f"DEBUG: Fixed {{context}} format in QA prompt")
                        
                    if "{{response}}" in template_text:
                        # Also replace 'response' with 'context' as needed
                        template_text = template_text.replace("{{response}}", "{context}")
                        print(f"DEBUG: Replaced {{response}} with {{context}} in QA prompt")
                            
                    print(f"DEBUG: Fixed placeholders in QA prompt")
                    self.qa_prompt = ChatPromptTemplate.from_template(template_text)
                else:
                    self.qa_prompt = qa_prompt_text
                
            # Load sample queries 
            self.sample_queries = prompts.get("sample_queries", [])
            
        except Exception as e:
            # Handle any errors in loading prompts
            print(f"ERROR: Failed to load prompts: {str(e)}")
            print(f"DEBUG: {traceback.format_exc()}")
            
            # Instead of using fallback prompts, raise the exception to fail explicitly
            # This makes debugging easier by exposing the actual error
            raise RuntimeError(f"Failed to load prompts for schema: {str(e)}") from e
            
    def _debug_print_prompts(self):
        """Print debug information about the loaded prompts"""
        print("\n=== DEBUG: PROMPT INFORMATION ===")
        print(f"Cypher prompt type: {type(self.cypher_prompt)}")
        print(f"QA prompt type: {type(self.qa_prompt)}")
        
        # Check prompt template variable names
        if hasattr(self.cypher_prompt, 'template') and hasattr(self.cypher_prompt.template, 'variable_names'):
            print(f"Cypher prompt variables: {self.cypher_prompt.template.variable_names}")
        elif hasattr(self.cypher_prompt, 'input_variables'):
            print(f"Cypher prompt variables: {self.cypher_prompt.input_variables}")
        else:
            print("Cannot determine Cypher prompt variables")
            
        if hasattr(self.qa_prompt, 'template') and hasattr(self.qa_prompt.template, 'variable_names'):
            print(f"QA prompt variables: {self.qa_prompt.template.variable_names}")
        elif hasattr(self.qa_prompt, 'input_variables'):
            print(f"QA prompt variables: {self.qa_prompt.input_variables}")
        else:
            print("Cannot determine QA prompt variables")
        print("===================================\n")

    def _format_schema(self):
        """
        Convert the class's schema property to a formatted text representation
        suitable for LLM prompts.
        
        Returns:
            str: A formatted text representation of the schema listing nodes and relationships
        """
        try:
            # Use the class's schema property
            schema = self.schema

            # Start building the schema text
            schema_text = "The graph contains the following nodes and relationships:\n\nNodes:\n"
            
            # Process nodes
            if 'nodes' in schema:
                for node in schema['nodes']:
                    node_label = node.get('label', 'UnknownNode')
                    schema_text += f"\n  {node_label}:\n"
                    
                    if 'properties' in node and node['properties']:
                        # Handle properties as a dictionary of key:type pairs
                        for prop_name, prop_type in node['properties'].items():
                            # Format property with type
                            schema_text += f"    {prop_name} ({prop_type})\n"
                    else:
                        schema_text += "    (no properties)\n"

            # Process relationships
            schema_text += "\nRelationships:\n"
            
            if 'relationships' in schema:
                for relationship in schema['relationships']:
                    rel_type = relationship.get('type', 'UNKNOWN_RELATIONSHIP')
                    # Handle different formats of relationship node references
                    start_node = None
                    end_node = None
                    
                    # Try different ways the schema might represent source/target nodes
                    if 'startNode' in relationship:
                        start_node = relationship['startNode']
                    elif 'source' in relationship:
                        start_node = relationship['source']
                    
                    if 'endNode' in relationship:
                        end_node = relationship['endNode']
                    elif 'target' in relationship:
                        end_node = relationship['target']
                    
                    schema_text += f"\n  [{start_node}]-[{rel_type}]->({end_node}):\n"
                    
                    if 'properties' in relationship and relationship['properties']:
                        # Handle properties as a dictionary of key:type pairs
                        for prop_name, prop_type in relationship['properties'].items():
                            # Format property with type
                            schema_text += f"    {prop_name} ({prop_type})\n"
                    else:
                        schema_text += "    (no properties)\n"
            
            self.formatted_schema = schema_text
            print(f"DEBUG: Successfully formatted schema")
        
        except Exception as e:
            print(f"ERROR: Failed to convert schema to text: {str(e)}")
            traceback.print_exc()
            # Set a fallback formatted schema
            self.formatted_schema = "Schema information not available in expected format."

    def _format_history(self) -> str:
        """Format last exchanges for context"""
        return "\n".join(
            f"{msg.content}" 
            for msg in self.history.messages[-5:] if msg.type == "ai"
        )

    @cacheable()
    def _extract_valid_cypher_query(self, llm_output: str) -> str:
        """
        Extract a valid Cypher query from the LLM's output.
        
        Args:
            llm_output: The raw output from the LLM containing Cypher query
            
        Returns:
            str: A clean, executable Cypher query
        """
        print(f"DEBUG: Extracting valid Cypher query from LLM output")
        print(f"DEBUG: Raw LLM output:\n{llm_output}")
        
        # List of valid Cypher keywords to check for
        valid_keywords = ["MATCH", "RETURN", "CREATE", "MERGE", "WITH", "CALL", "OPTIONAL", "UNWIND"]
        
        # Check if the output contains 'cypher query:' pattern
        try:
            match = re.search(r'cypher query:\s*(.+?)(?:\n|$)', llm_output, re.IGNORECASE)
            if match:
                query = match.group(1).strip()
                print(f"DEBUG: Found query using 'cypher query:' pattern: {query}")
                return query
        except Exception as e:
            print(f"WARNING: Error in 'cypher query:' pattern matching: {str(e)}")
        
        # Look for numbered queries in the output (e.g., "1. MATCH (n) RETURN n")
        try:
            # First, remove sections like "Simple Query:" or "Medium Query:" or "Complex Query:"
            # by splitting the text into sections and processing each section
            sections = re.split(r'(Simple|Medium|Complex)\s+Query:', llm_output)
            
            for i in range(1, len(sections), 2):  # Process each section after a header
                if i+1 < len(sections):
                    section_text = sections[i+1]
                    # Look for numbered queries in this section
                    numbered_matches = re.findall(r'\d+\.\s*(`?)([^`\n]+)(`?)', section_text)
                    for match in numbered_matches:
                        query_text = match[1].strip()
                        # Check if it starts with a valid Cypher keyword
                        if any(query_text.upper().startswith(keyword) for keyword in valid_keywords):
                            print(f"DEBUG: Found numbered query: {query_text}")
                            return query_text
        except Exception as e:
            print(f"WARNING: Error in numbered query extraction: {str(e)}")
        
        # Look for content within backticks
        try:
            matches = re.findall(r'`([^`]+)`', llm_output)
            if matches:
                # Find the first match that starts with a valid Cypher keyword
                for match in matches:
                    cleaned = match.strip()
                    if any(cleaned.upper().startswith(keyword) for keyword in valid_keywords):
                        print(f"DEBUG: Found query in backticks: {cleaned}")
                        return cleaned
        except Exception as e:
            print(f"WARNING: Error in backtick pattern matching: {str(e)}")
        
        # If we can't find a pattern, try to extract a valid Cypher query based on keywords
        try:
            lines = llm_output.split('\n')
            for line in lines:
                cleaned = line.strip()
                if any(cleaned.upper().startswith(keyword) for keyword in valid_keywords):
                    print(f"DEBUG: Found query by keyword: {cleaned}")
                    return cleaned
        except Exception as e:
            print(f"WARNING: Error in keyword extraction: {str(e)}")
        
        # If all else fails, return a simple query
        print(f"WARNING: Could not extract a valid Cypher query from LLM output. Using fallback query.")
        return "MATCH (n) RETURN labels(n) as labels, count(n) as count LIMIT 10"

    def _safe_execute_query(self, graph, query):
        """Safely execute a Neo4j query without risking recursion"""
        try:
            # Use the original query method directly
            original_query = graph.__class__.query
            return original_query(graph, query)
        except Exception as e:
            print(f"ERROR: Query execution failed: {str(e)}")
            return [{"error": f"Query failed: {str(e)}"}]

    def query(self, question: str) -> Dict[str, Any]:
        """Process user queries against the knowledge graph"""
        try:
            print(f"\n===== DEBUG: PROCESSING QUERY: '{question}' =====")
            # Add detailed diagnostic info about prompts
            self._debug_print_prompts()
            
            # Execute chain with context
            hist = self._format_history()
            print(f"HISTORY TO BE USED: {hist}")
            
            # Add schema awareness to the query
            start_time = time.time()
            
            # Handle schema-related queries directly
            if any(keyword in question.lower() for keyword in ['schema', 'structure', 'entities', 'nodes', 'relationships']):
                try:
                    # Get node information
                    node_query = """
                    MATCH (n)
                    WITH DISTINCT labels(n) as labels, count(n) as count
                    RETURN {labels: labels, count: count} as nodeInfo
                    """
                    node_info = self._safe_execute_query(self.chain.graph, node_query)
                    
                    # Get relationship information
                    rel_query = """
                    MATCH ()-[r]->() 
                    WITH DISTINCT type(r) as type, count(r) as count
                    RETURN {type: type, count: count} as relInfo
                    """
                    rel_info = self._safe_execute_query(self.chain.graph, rel_query)
                    
                    # Format results nicely
                    nodes_str = "\nNodes:\n" + "\n".join([f"- {info['nodeInfo']['labels']}: {info['nodeInfo']['count']} instances" for info in node_info])
                    rels_str = "\nRelationships:\n" + "\n".join([f"- {info['relInfo']['type']}: {info['relInfo']['count']} instances" for info in rel_info])
                    
                    return {"result": f"Here's the graph structure:{nodes_str}{rels_str}"}

                except Exception as schema_e:
                    print(f"ERROR: Schema query failed: {str(schema_e)}")
                    # Fall through to regular processing
            
            # Use a direct approach to generate and execute Cypher
            try:
                # Step 1: Generate Cypher using the cypher_prompt
                cypher_gen_inputs = {"query": question}
                if hasattr(self.chain, "cypher_llm") and hasattr(self.chain, "cypher_prompt"):
                    print("DEBUG: Directly generating Cypher using the cypher prompt")
                    generated_cypher = self.chain.cypher_llm.invoke(
                        self.chain.cypher_prompt.format(**cypher_gen_inputs)
                    ).content
                    print(f"DEBUG: Generated Cypher:\n{generated_cypher}")
                    # Check if the Cypher query is None or empty
                    if generated_cypher is None or generated_cypher.strip() == "":
                        print("DEBUG: Generated Cypher query is None or empty")
                        return {"result": "Not able to prepare valid queries. Update your queries with appropriate data attributes"}

                    # Step 2: Extract valid Cypher query
                    clean_cypher = self._extract_valid_cypher_query(generated_cypher)


                    try:
                        print(f"DEBUG: Executing extracted Cypher: {generated_cypher}")
                        context = self._safe_execute_query(self.chain.graph, clean_cypher)
                    except Exception as exec_error:
                        print(f"DEBUG: Error executing extracted query: {str(exec_error)}")
                        # Try to extract a different query if the first one fails
                        fallback_cypher = "MATCH (n) RETURN labels(n) as labels, count(n) as count LIMIT 10"
                        print(f"DEBUG: Falling back to simple query: {fallback_cypher}")
                        context = self._safe_execute_query(self.chain.graph, fallback_cypher)
                    
                    # Step 4: Generate the final answer using the qa_prompt
                    qa_inputs = {"query": question, "context": context}
                    if hasattr(self.chain, "qa_llm") and hasattr(self.chain, "qa_prompt"):
                        print("DEBUG: Generating answer using QA prompt")
                        answer = self.chain.qa_llm.invoke(
                            self.chain.qa_prompt.format(**qa_inputs)
                        ).content
                        result = {"result": answer}
                    else:
                        result = {"result": f"Query results: {context}"}
                else:
                    # Fallback to standard chain invocation
                    print("DEBUG: Falling back to standard chain invocation")
                    try:
                        result = self.chain.invoke({"query": question})
                    except Exception as chain_error:
                        print(f"DEBUG: Standard chain invocation failed: {str(chain_error)}")
                        # Check if error is related to None Cypher query
                        if "Invalid input 'None'" in str(chain_error):
                            return {"result": "Not able to prepare valid queries. Please update your question with specific data attributes or relationships."}
                        # Otherwise, raise the error to be caught by the outer try-except
                        raise
            except Exception as e:
                print(f"ERROR: Direct approach failed: {str(e)}")
                print(f"DEBUG: {traceback.format_exc()}")
                
                # Fallback to a simple query if everything else fails
                try:
                    print("DEBUG: Falling back to simple query")
                    fallback_query = "MATCH (n) RETURN labels(n) as labels, count(n) as count LIMIT 10"
                    context = self._safe_execute_query(self.chain.graph, fallback_query)
                    result = {
                        "result": f"I encountered an error processing your query. \n\nHere's some basic information about the graph: {context}"
                    }
                except Exception as e2:
                    print(f"ERROR: Even simple fallback failed: {str(e2)}")
                    result = {
                        "result": f"I encountered multiple errors processing your query. The database might be unavailable or the query was malformed."
                    }
            
            query_time = time.time() - start_time
            print(f"Query completed in {query_time:.2f} seconds")
            
            # Persist conversation
            print(f"Persisting conversation: {result}")
            self.history.add_user_message(question)
            self.history.add_ai_message(result["result"])
            
            return result
        
        except Exception as e:
            print(f"Research error: {str(e)}")
            print(f"Full error traceback: {traceback.format_exc()}")
            return {"result": f"An error occurred while processing your query: {str(e)}. Please try again or contact support."}

    def __del__(self):
        # Close the cache connection when the object is garbage collected
        if hasattr(self, 'cache'):
            self.cache.close()

# Singleton-like behavior with dict of assistants by db_id
_assistants = {}
_lock = threading.Lock()

def get_schema_aware_assistant(db_id: str, schema_id: str, schema: str, session_id: str = None) -> SchemaAwareGraphAssistant:
    """
    Get a schema-aware assistant for the specified database ID.
    Creates a new assistant if one doesn't exist, otherwise returns the existing one.
    
    Args:
        db_id: ID of the Neo4j database to query
        schema_id: ID of the schema being used
        schema: JSON string containing the schema
        session_id: Optional session ID for chat history
    Returns:
        SchemaAwareGraphAssistant: The assistant for the database
    """
    # Create a unique key combining db_id, schema_id and session_id
    key = f"{db_id}:{schema_id}:{session_id}" if session_id else f"{db_id}:{schema_id}"
    
    try:
        with _lock:
            if key not in _assistants:
                # Log connection attempt for debugging
                print(f"DEBUG: Creating new schema-aware assistant for {db_id} with schema {schema_id}")
                print(f"DEBUG: Schema: {schema}")
                _assistants[key] = SchemaAwareGraphAssistant(db_id, schema_id, schema, session_id)
                print(f"DEBUG: Successfully created assistant for {db_id}")
            return _assistants[key]
    except Exception as e:
        # Log the error with detailed information
        error_message = f"Error creating schema-aware assistant for {db_id}: {str(e)}"
        print(f"ERROR: {error_message}")
        
        # Check for specific error types to provide better feedback
        if "Could not use APOC procedures" in str(e):
            print("DEBUG: APOC plugin issue detected. Ensure APOC is installed and configured.")
        elif "authentication failed" in str(e).lower() or "unauthorized" in str(e).lower():
            print("DEBUG: Authentication failed. Check Neo4j credentials.")
        elif "connection refused" in str(e).lower() or "unreachable" in str(e).lower():
            print("DEBUG: Connection failed. Check Neo4j server availability and network settings.")
            
        # Re-raise with a more informative message
        raise RuntimeError(f"Failed to initialize schema-aware assistant: {error_message}") from e
